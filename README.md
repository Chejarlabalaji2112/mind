A simultation available 

# Companion desgin
# when closed
![Hitomi Closed](./src/mind/assets/images/closed_cone.png)
# boot_up
![Hitomi Closed](./src/mind/assets/images/opened_name_cone.png)
# running
![Hitomi opened](./src/mind/assets/images/opened_eyes_cone.png)

# UI demo
<video src="./src/mind/assets/videos/ui_demo.mp4" width="640" controls> not supported</video>
---

# ğŸ¤– Personalized Companion â€” HITOMI Architecture

> _Adaptive Interaction Â· Emotional Understanding Â· Causal Intelligence_

---

## ğŸ§  Overview

**HITOMI** is the orchestrator agent powering a **Personalized Companion** designed for adaptive interaction, emotional understanding, and causal reasoning.  
The architecture follows a **Ports & Adapters** (Hexagonal) model â€” ensuring modularity, transparency, and integration between perception, cognition, and action.

---

## ğŸ§© System Architecture
![Hitomi Architecture](./src/mind/assets/images/architecture.png)
Above image is a base architecture. New sub-agents will be added.

### ğŸ”¸ Core Concept
The system is divided into **Ports** (interfaces for perception and action) and **Agents** (reasoning and monitoring units).  
HITOMI acts as the **main orchestrator**, coordinating multiple sub-agents to perceive, interpret, and respond intelligently to user states and environmental cues.

---

### ğŸ§  Components

#### **1. HITOMI (Orchestrator / Main Agent)**
- Central cognitive controller that manages reasoning and decision flow.  
- Interfaces directly with the **User**, orchestrating all sub-modules.  
- Maintains adaptive behavior through continuous feedback.

#### **2. Perception Port**
- Handles sensory input from the **Environment** and **User**.  
- Integrates computer vision, speech processing, and context inference.  
- Connects to:
  - **Memory** â€” for persistent state storage.  
  - **Tools** â€” for reasoning, planning, and API calls.

#### **3. Act Port**
- Executes high-level commands decided by HITOMI.  
- Manages actuators or simulated control via **ROS2** and **Gazebo**.  
- Provides feedback to both the environment and HITOMI.

#### **4. Sub-Agents**
| Module | Function |
|--------|-----------|
| ğŸ§­ **User Intent Monitor** | Interprets user goals and conversational context. |
| ğŸ­ **User Emotions Monitor** | Analyzes emotional tone and adjusts responses. |
| â¤ï¸ **Health Monitor** | Tracks user well-being and safety signals. |
| âš™ï¸ **Utilities Manager** | Manages reminders, scheduling, and background tasks. |

#### **5. Memory & Tools**
- **Memory:** Long-term storage of user data, context, and emotional states.  
- **Tools:** Interfaces for reasoning APIs, retrieval systems, and task automation.

---

## âš™ï¸ Tech Stack

**Languages:** Python, C++  
**Frameworks:**PyTorch, OpenCV, LangChain, LangGraph  
**Libraries:** SpeechRecognition, pyttsx3, Transformers  
**Tools:** VS Code, Jupyter, Git, Anaconda

---

## ğŸš€ Features

- ğŸ—£ï¸ Natural voice-to-voice interaction  
- ğŸ­ Emotion-aware dialogue  
- ğŸ§  Causal reasoning and adaptive response  
- ğŸ§© Modular architecture for easy extension  
- ğŸ§ª Simulation-ready with ROS2 + Gazebo integration  

---

## ğŸ§ª Research Direction

This system explores how **Cognitive Architectures** and **World Models** can be integrated into companion robots.  
HITOMIâ€™s goal is not only to interact but to **understand** â€” modeling user intent, emotion, and environment through causal inference rather than statistical correlation.


